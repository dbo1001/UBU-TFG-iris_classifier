{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de los directorios de entrenamiento y testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\theac\\\\Desktop\\\\Repo TFG\\\\notebooks'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, shutil\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directorio con el datset de entrenamiento y testo\n",
    "test_train_dir = os.path.join(os.getcwd(), 'test_train')\n",
    "os.mkdir(test_train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directorio de entrenamiento\n",
    "train_dir = os.path.join('test_train', 'train')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "# directorio de validación\n",
    "val_dir = os.path.join('test_train', 'validation')\n",
    "os.mkdir(val_dir)\n",
    "\n",
    "# directorio de testeo\n",
    "test_dir = os.path.join('test_train', 'test')\n",
    "os.mkdir(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si fuese un problema de clasificación binaria únicamente habrían 2 clases, pero al tratarse de clasificación multiclase se necesitan más de 2 clases que en este caso se corresponden con el número de sujetos que tenemos en nuestra base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sujetos = len(os.listdir('CASIA-IrisV1')) # nro de clases/sujetos\n",
    "sujetos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso lo que se hará será coger las muestras de la sesión 1 (3 muestras) para entrenar la red y las de la sesión 2 (4 muestras) para la validación y testeo(2 y 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, sujetos+1): # nombrar sujetos del 1 al 108\n",
    "    # subdirectorios de entrenamiento con 108 clases\n",
    "    os.mkdir(os.path.join(train_dir, 'sujeto_' + str(i)))\n",
    "    \n",
    "    # subdirectorio de testeo con 108 clases\n",
    "    os.mkdir(os.path.join(val_dir, 'sujeto_' + str(i)))\n",
    "\n",
    "    # subdirectorio de testeo con 108 clases\n",
    "    os.mkdir(os.path.join(test_dir, 'sujeto_' + str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree('test_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordenaremos cada una de las muestras en sus correspondientes directorios\n",
    "j = 1\n",
    "for file in os.listdir('CASIA-IrisV1'):\n",
    "    path_to_session = 'CASIA-IrisV1' + os.sep + file\n",
    "    for i, session in enumerate(os.listdir(path_to_session)):\n",
    "        src = path_to_session + os.sep + session\n",
    "        for eye in os.listdir(src):\n",
    "            if i == 0: # copiamos al directorio de entrenamiento\n",
    "                # pertenece a la sesión 1\n",
    "                shutil.copy(src + os.sep + eye, os.path.join(train_dir, 'sujeto_' + str(j)))\n",
    "            else:\n",
    "                for k, file in enumerate(os.listdir(src)):\n",
    "                    temp_src = src + os.sep + file\n",
    "                    if k < 2:\n",
    "                        #copiamos al directorio de validación\n",
    "                        shutil.copy(temp_src, os.path.join(val_dir, 'sujeto_' + str(j)))\n",
    "                    else:\n",
    "                        # copiamos al directorio de testeo\n",
    "                        shutil.copy(temp_src, os.path.join(test_dir, 'sujeto_' + str(j)))\n",
    "\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestras de entrenamiento totales: 324\n",
      "Muestras de validación totales: 216\n",
      "Muestras de testeo totales: 216\n"
     ]
    }
   ],
   "source": [
    "print('Muestras de entrenamiento totales:', len(os.listdir(train_dir))*3) # *3 muestras por sujeto en sesión 1\n",
    "print('Muestras de validación totales:', len(os.listdir(val_dir))*2) # *2 muestras por sujeto en sesión 2\n",
    "print('Muestras de testeo totales:', len(os.listdir(test_dir))*2)  # *2 muestras por sujeto en sesión 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al crear el set de validación hacemos que la proporción para el entrenamiento sea más apropiada, ya que dedicaremos la mayor parte de las muestras para entrenar.\n",
    "\n",
    "Y lo que se ha hecho pare el resto es dividirlas , tenemos 4 muestras de cada ojo tomadas en las segunda sesión por lo que cogeremos la mitad para validar y la otra mitad para testear.\n",
    "De este modo, tendremos un 43%, un 28% para validar y otro 28% para testear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usará un modelo previamente entrenado ya que resulta conveniente cuando el dataset con el que se cuenta es pequeño.\n",
    "\n",
    "Un modelo preentrenado no es más que una red que ya ha sido entrenada con un conjunto de datos enorme, )en tareas de clasifición de imágenes por lo general) y que posteriormente ha sido guardada.\n",
    "\n",
    "En nuestro caso consideraremos una red entrenada con el dataset de ImageNet la cual consta de 1,4 millones de imágnes etiquetdas y 1000 clases diferentes.\n",
    "\n",
    "Se usará el modelo VGG-16, desarrolado por Karen Simonyan and Andrew Zisserman in 2014. Aunque se trata de un modelo antiguo se ha optado por este por la facilidad de uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [224, 224]\n",
    "\n",
    "# modelo\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3],\n",
    "           weights='imagenet',\n",
    "           include_top=False)\n",
    "\n",
    "# pesos ya entrenados\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = Flatten()(vgg.output)\n",
    "classes = sujetos\n",
    "\n",
    "# última capa de la red, ajustamos la salida al número de clases\n",
    "prediction = Dense(classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 108)               2709612   \n",
      "=================================================================\n",
      "Total params: 17,424,300\n",
      "Trainable params: 2,709,612\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "\n",
    "# visualizar red\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam', # probar con otro optimizador y comparar\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_train\\\\train'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = os.path.join('test_train', 'train')\n",
    "valid_path = os.path.join('test_train', 'validation')\n",
    "test_path = os.path.join('test_train', 'test')\n",
    "train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 324 images belonging to 108 classes.\n",
      "Found 216 images belonging to 108 classes.\n",
      "Epoch 1/10\n",
      "11/11 [==============================] - 100s 9s/step - loss: 7.8274 - accuracy: 0.0370 - val_loss: 5.1250 - val_accuracy: 0.1389\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 101s 9s/step - loss: 4.7223 - accuracy: 0.1636 - val_loss: 3.2636 - val_accuracy: 0.3519\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 105s 10s/step - loss: 2.7561 - accuracy: 0.4475 - val_loss: 2.2586 - val_accuracy: 0.5093\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 99s 9s/step - loss: 1.8992 - accuracy: 0.6019 - val_loss: 1.7200 - val_accuracy: 0.6204\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 97s 9s/step - loss: 1.3121 - accuracy: 0.7130 - val_loss: 1.4546 - val_accuracy: 0.6852\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 101s 9s/step - loss: 0.7919 - accuracy: 0.8549 - val_loss: 1.2556 - val_accuracy: 0.7222\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 107s 10s/step - loss: 0.5687 - accuracy: 0.8827 - val_loss: 1.0767 - val_accuracy: 0.7546\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 107s 10s/step - loss: 0.4484 - accuracy: 0.9290 - val_loss: 1.0668 - val_accuracy: 0.7546\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 101s 9s/step - loss: 0.3506 - accuracy: 0.9321 - val_loss: 0.9057 - val_accuracy: 0.7778\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 104s 9s/step - loss: 0.3024 - accuracy: 0.9352 - val_loss: 0.9025 - val_accuracy: 0.7917\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# train_path = os.path.join(path_dataset_test_train, 'train')\n",
    "# valid_path = os.path.join(path_dataset_test_train, 'test')\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_generator = train_datagen.flow_from_directory(train_path,\n",
    "                                                target_size=IMAGE_SIZE,\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='categorical')\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(valid_path,\n",
    "                                            target_size=IMAGE_SIZE,\n",
    "                                            batch_size=32,\n",
    "                                            class_mode='categorical')\n",
    "history = model.fit_generator(training_generator,\n",
    "                              validation_data=valid_generator,\n",
    "                              epochs=10, # subir a 10 a ver si mejora\n",
    "                              steps_per_epoch=len(training_generator),\n",
    "                              validation_steps=len(valid_generator))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
